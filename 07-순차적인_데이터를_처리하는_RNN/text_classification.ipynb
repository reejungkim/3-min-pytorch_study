{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "text_classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I04jekS4fBut"
      },
      "source": [
        "# 프로젝트 1. 영화 리뷰 감정 분석\n",
        "**RNN 을 이용해 IMDB 데이터를 가지고 텍스트 감정분석을 해 봅시다.**\n",
        "\n",
        "이번 책에서 처음으로 접하는 텍스트 형태의 데이터셋인 IMDB 데이터셋은 50,000건의 영화 리뷰로 이루어져 있습니다.\n",
        "각 리뷰는 다수의 영어 문장들로 이루어져 있으며, 평점이 7점 이상의 긍정적인 영화 리뷰는 2로, 평점이 4점 이하인 부정적인 영화 리뷰는 1로 레이블링 되어 있습니다. 영화 리뷰 텍스트를 RNN 에 입력시켜 영화평의 전체 내용을 압축하고, 이렇게 압축된 리뷰가 긍정적인지 부정적인지 판단해주는 간단한 분류 모델을 만드는 것이 이번 프로젝트의 목표입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73voP5VTfBuv"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data, datasets \n",
        "#자연어 데이터 처리를 위해서 torchtext 임포트\n",
        "\n",
        "# 기존 코드에는 TorchText 0.8.0으로 셋팅되어 있어서 from torchtext import data, datasets로 명시 되어 있는데 \n",
        "# 코랩에 깔려있는 TorchText의 경우, 0.9.0 버전이기에 from torchtext를 torchtext.legacy로 변경해주어야 한다."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdpgzUt2fBuw",
        "outputId": "5e9b0515-01db-458f-b230-762427d3221d"
      },
      "source": [
        "# 하이퍼파라미터 정의\n",
        "BATCH_SIZE = 64\n",
        "lr = 0.001\n",
        "EPOCHS = 10\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")#GPU, CPU\n",
        "print(\"다음 기기로 학습합니다:\", DEVICE)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "다음 기기로 학습합니다: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "wqfwglArfBuy",
        "outputId": "557de4be-a88e-4283-ed05-0273d9cf2031"
      },
      "source": [
        "# 데이터 로딩하기\n",
        "print(\"데이터 로딩중...\")\n",
        "\n",
        "# data.Field 설명 #\n",
        "# sequential인자 : TEXT는 Sequential 데이터라 True, Lable은 비Sequential이라 False로 설정\n",
        "# batch_first : Batch를 우선시 하여, Tensor 크기를 (BATCH_SIZE, 문장의 최대 길이)로 설정\n",
        "# lower : 소문자 전환 인자\n",
        "# # # # # # # # # #\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)\n",
        "\n",
        "#IMDB 데이터 로딩\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "# data.Field.build_vocab() 라이브러리\n",
        "# 문장 내 단어와 Integer index 를 매칭시키는 단어장(vocab)을 생성 == 워드 임베딩을 위한 Vocab 생성\n",
        "# <UNK> = 0, <PAD> = 1 토큰도 추가.\n",
        "# min_freq : 최소 5번 이상 등장한 단어들만 사전에 담겠다는 것. \n",
        "# 5번 미만으로 등장하는 단어는 UNK라는 토큰으로 대체\n",
        "TEXT.build_vocab(trainset, min_freq=5)# TEXT 데이터를 기반으로 Vocab 생성\n",
        "LABEL.build_vocab(trainset)# LABEL 데이터를 기반으로 Vocab 생성\n",
        "\n",
        "\n",
        "# 학습용 데이터를 학습셋 80% 검증셋 20% 로 나누기\n",
        "trainset, valset = trainset.split(split_ratio=0.8)\n",
        "# 매 배치마다 비슷한 길이에 맞춰 줄 수 있도록 iterator 정의\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "        (trainset, valset, testset), batch_size=BATCH_SIZE,\n",
        "        shuffle=True, repeat=False)\n",
        "\n",
        "\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2 \n",
        "# Positive, Negative Class가 두 개\n",
        "\n",
        "'''\n",
        "# Test #\n",
        "for batch in train_iter:\n",
        "    print(batch.text)\n",
        "    print(batch.label)\n",
        "    break\n",
        "'''     "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터 로딩중...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Test #\\nfor batch in train_iter:\\n    print(batch.text)\\n    print(batch.label)\\n    break\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fBT0CkzfBuz",
        "outputId": "c32b5701-94f6-460a-a26a-85eff2e31165"
      },
      "source": [
        "print(\"[학습셋]: %d [검증셋]: %d [테스트셋]: %d [단어수]: %d [클래스] %d\"\n",
        "      % (len(trainset),len(valset), len(testset), vocab_size, n_classes))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[학습셋]: 20000 [검증셋]: 5000 [테스트셋]: 25000 [단어수]: 46159 [클래스] 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNvsLlbUfBu0"
      },
      "source": [
        "class BasicGRU(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "        super(BasicGRU, self).__init__()\n",
        "        print(\"Building Basic GRU model...\")\n",
        "        self.n_layers = n_layers # 일반적으로는 2\n",
        "\n",
        "        #n_vocab : Vocab 안에 있는 단어의 개수, embed_dim : 임베딩 된 단어 텐서가 갖는 차원 값(dimension)\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "\n",
        "        # hidden state vector의 dimension과 dropout 정의\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        #앞에서 정의한 하이퍼 파라미터를 넣어 GRU 정의\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
        "                          num_layers=self.n_layers,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        #Input: GRU의 hidden state vector(context), Output : Class probability vector\n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input data: 한 batch 내 모든 영화 평가 데이터\n",
        "        \n",
        "        x = self.embed(x)# 영화 평 임베딩\n",
        "\n",
        "        h_0 = self._init_state(batch_size=x.size(0)) # 초기 hidden state vector를 zero vector로 생성\n",
        "        x, _ = self.gru(x, h_0)  # [i, b, h] 출력값 :  (batch_size, 입력 x의 길이, hidden_dim)\n",
        "\n",
        "        # h_t : Batch 내 모든 sequential hidden state vector의 제일 마지막 토큰을 내포한 (batch_size, 1, hidden_dim)형태의 텐서 추출\n",
        "        # 다른 의미로 영화 리뷰 배열들을 압축한 hidden state vector\n",
        "        h_t = x[:,-1,:]\n",
        "\n",
        "        self.dropout(h_t)# dropout 설정 후, \n",
        "\n",
        "        # linear layer의 입력으로 주고, 각 클래스 별 결과 logit을 생성.\n",
        "        logit = self.out(h_t)  # [b, h] -> [b, o]\n",
        "        return logit\n",
        "    \n",
        "    def _init_state(self, batch_size=1):\n",
        "        weight = next(self.parameters()).data\n",
        "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXNPaXKDfBu0"
      },
      "source": [
        "def train(model, optimizer, train_iter):\n",
        "    model.train()\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        y.data.sub_(1)  # 레이블 값을 (기존 1, 2)에서 0과 1로 변환\n",
        "        optimizer.zero_grad()# 매번 기울기를 새로 계산하기 위해서 zero 로 초기화\n",
        "        logit = model(x)#모델의 예측값 logit 계산\n",
        "        loss = F.cross_entropy(logit, y)# logit과 실제 label간의 오차를 구하고 기울기 계산\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD2EH8NmfBu0"
      },
      "source": [
        "def evaluate(model, val_iter):\n",
        "    \"\"\"evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    corrects, total_loss = 0, 0\n",
        "    for batch in val_iter:# Validation 데이터셋에 대하여 \n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
        "        total_loss += loss.item()\n",
        "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "    #전체 validation 셋에 대한 평균 loss와 accuracy를 구하는 과정\n",
        "    size = len(val_iter.dataset)\n",
        "    avg_loss = total_loss / size\n",
        "    avg_accuracy = 100.0 * corrects / size\n",
        "    return avg_loss, avg_accuracy"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKBnEOeRfBu1",
        "outputId": "2e75e522-482d-4aa1-c2eb-cdfa58006aba"
      },
      "source": [
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Basic GRU model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KkkqAK5fBu1",
        "outputId": "73bf3f04-de10-4178-b41f-a8fd4f7a3329"
      },
      "source": [
        "best_val_loss = None\n",
        "for e in range(1, EPOCHS+1):\n",
        "    train(model, optimizer, train_iter)\n",
        "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
        "\n",
        "    print(\"[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (e, val_loss, val_accuracy))\n",
        "    \n",
        "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        if not os.path.isdir(\"snapshot\"):\n",
        "            os.makedirs(\"snapshot\")\n",
        "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
        "        best_val_loss = val_loss"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[이폭: 1] 검증 오차: 0.69 | 검증 정확도:52.28\n",
            "[이폭: 2] 검증 오차: 0.69 | 검증 정확도:52.50\n",
            "[이폭: 3] 검증 오차: 0.68 | 검증 정확도:56.70\n",
            "[이폭: 4] 검증 오차: 0.43 | 검증 정확도:80.98\n",
            "[이폭: 5] 검증 오차: 0.31 | 검증 정확도:86.58\n",
            "[이폭: 6] 검증 오차: 0.34 | 검증 정확도:86.44\n",
            "[이폭: 7] 검증 오차: 0.37 | 검증 정확도:86.08\n",
            "[이폭: 8] 검증 오차: 0.43 | 검증 정확도:86.92\n",
            "[이폭: 9] 검증 오차: 0.47 | 검증 정확도:86.10\n",
            "[이폭: 10] 검증 오차: 0.49 | 검증 정확도:86.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqeJ7anVfBu2",
        "outputId": "0a38ca7f-8217-4f10-a791-2231266135f5"
      },
      "source": [
        "model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iter)\n",
        "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "테스트 오차:  0.31 | 테스트 정확도: 86.98\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}